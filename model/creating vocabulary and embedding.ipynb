{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have text pre-processed, we can create the vocabulary and embedding.\n",
    "\n",
    "There are many different options, which embedding to use - for example doc2vec, glove, fastext, .. for the current first iteration of our project, we are going to go with Facebook's fastext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# df = pd.read_csv('../data/train/comments_processed.csv', index_col=0)\n",
    "# df_test = pd.read_csv('../data/test/comments_processed.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train/comments_processed_v2.csv', index_col=0)\n",
    "df_test = pd.read_csv('../data/test/comments_processed_v2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.comment = df.comment.apply(literal_eval)\n",
    "df_test.comment = df_test.comment.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[for, movi, that, get, respect, there, sure, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bizarr, horror, movi, fill, with, famou, face...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[solid, unremark, film, matthau, einstein, won...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[strang, feel, sit, alon, theater, occupi, par...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, probabl, all, alreadi, know, thi, now, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment\n",
       "0  [for, movi, that, get, respect, there, sure, a...          1\n",
       "1  [bizarr, horror, movi, fill, with, famou, face...          1\n",
       "2  [solid, unremark, film, matthau, einstein, won...          1\n",
       "3  [strang, feel, sit, alon, theater, occupi, par...          1\n",
       "4  [you, probabl, all, alreadi, know, thi, now, b...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the vocabulary by assigning every word its number. \n",
    "However, we will sort the words according to their \"priority\", which is simply number of word occurences. \n",
    "This is only being done for the purpose of potentially limiting the size of vocabulary (e.g. to 15000 most frequently used words). We will see, if it makes sense later.\n",
    "\n",
    "Also, another option is to use already built vocabulary along with already trained and prepared embeddings. \n",
    "This has an advantage of covering most of the words for english language. However, there might be slight disadvantage that the embedding is not domain specific - and people might communicate movie reviews a little bit differently than other texts. \n",
    "\n",
    "We can compare mulitple approaches later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary\n",
    "words = {}\n",
    "for idx, comment in df.comment.iteritems():\n",
    "    for word in comment:\n",
    "        if word in words.keys():\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order by occurences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [(k, v) for k, v in words.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(words, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 334779),\n",
       " ('and', 162257),\n",
       " ('thi', 75456),\n",
       " ('that', 73306),\n",
       " ('not', 63580),\n",
       " ('movi', 50595),\n",
       " ('film', 47239),\n",
       " ('for', 44022),\n",
       " ('with', 43951),\n",
       " ('but', 41765),\n",
       " ('have', 31024),\n",
       " ('are', 30243),\n",
       " ('you', 29637),\n",
       " ('one', 26912),\n",
       " ('all', 23387),\n",
       " ('like', 22185),\n",
       " ('who', 21099),\n",
       " ('they', 20965),\n",
       " ('from', 20439),\n",
       " ('there', 18591),\n",
       " ('her', 18401),\n",
       " ('just', 17638),\n",
       " ('about', 17322),\n",
       " ('out', 16561),\n",
       " ('what', 16082),\n",
       " ('some', 15686),\n",
       " ('time', 15542),\n",
       " ('good', 14924),\n",
       " ('can', 14569),\n",
       " ('make', 14546)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('firsthalf', 1),\n",
       " ('drosselmei', 1),\n",
       " ('closedforweekend', 1),\n",
       " ('highsecur', 1),\n",
       " ('warveteran', 1),\n",
       " ('milla', 1),\n",
       " ('antithril', 1),\n",
       " ('descriptionwoman', 1),\n",
       " ('securityi', 1),\n",
       " ('realizationi', 1),\n",
       " ('quarterfin', 1),\n",
       " ('rakishli', 1),\n",
       " ('untempt', 1),\n",
       " ('crumley', 1),\n",
       " ('buic', 1),\n",
       " ('antiplot', 1),\n",
       " ('fictiondrama', 1),\n",
       " ('thereinaft', 1),\n",
       " ('lockup', 1),\n",
       " ('dresssuit', 1),\n",
       " ('overvot', 1),\n",
       " ('ontyp', 1),\n",
       " ('infantalis', 1),\n",
       " ('rou', 1),\n",
       " ('orientalist', 1),\n",
       " ('tooand', 1),\n",
       " ('repleat', 1),\n",
       " ('jowl', 1),\n",
       " ('camora', 1),\n",
       " ('capich', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79771"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As probably expected, we can see that for example occurence of word \"movi\" (stem from \"movie\") is almost the same as number of documents (even higher). So it goes for word film. I think these two words might be domain stopwords - frequently used and there with no added value to comments. \n",
    "\n",
    "What is interesting is that words \"good\" and \"like\" made it pretty high. However, they might have been used with not - so they alone (without context) are not helpful (this might be an example of why \"not\" should not be in the englighs stopwords for this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing is that we surely have a lot of words, that occur once in all the comments. \n",
    "Our vocabulary is on the other hand quite large - almost 80 000. \n",
    "There comes the question, if it makes sense to leave every word in vocabulary - or if we want to use only words that occur for example at least 2-3 times (or more). \n",
    "\n",
    "Further analysis, like tf-idf computing, might help us understand the most meaningful words for our model, but for now, we will just skip words that occured a little times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aubrey', 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[12500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inter', 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[15001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('phedon', 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('serrador', 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[35000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can also see that our vocab also consist of typos. \n",
    "After first most frequent 15000 words we can see that words occur no more than 9 times (from the whole set of 25000 movies). We can here try to limit our vocab to 15000 most ferquent words (with the risk od decreasing accuracy). \n",
    "\n",
    "We can also play with this parameter later - this is now done to simplyfy things a liitle and to speed up training as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "# assign words a number\n",
    "for idx, word in enumerate(words):\n",
    "    vocab[word[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('and', 2),\n",
       " ('thi', 3),\n",
       " ('that', 4),\n",
       " ('not', 5),\n",
       " ('movi', 6),\n",
       " ('film', 7),\n",
       " ('for', 8),\n",
       " ('with', 9),\n",
       " ('but', 10),\n",
       " ('have', 11),\n",
       " ('are', 12),\n",
       " ('you', 13),\n",
       " ('one', 14),\n",
       " ('all', 15),\n",
       " ('like', 16),\n",
       " ('who', 17),\n",
       " ('they', 18),\n",
       " ('from', 19),\n",
       " ('there', 20)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for k, v in vocab.items()][:20]  # not the most efective way :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vocabulary constructed, we can replace words with their ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_ids'] = df.comment.apply(lambda comm: list(map(lambda x: vocab.get(x, None), comm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we created the vocabulary ourselves and on train data set only, here can happen that test data set contains words that are new (not present in train data set). We will remove the words from comments as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['comment_ids'] = df_test.comment.apply(lambda comm: list(map(lambda x: vocab.get(x, None), comm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_ids'] = df.comment_ids.apply(lambda comm: list(filter(lambda x: x is not None, comm)))\n",
    "df_test['comment_ids'] = df_test.comment_ids.apply(lambda comm: list(filter(lambda x: x is not None, comm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[base, actual, stori, john, boorman, show, the...</td>\n",
       "      <td>1</td>\n",
       "      <td>[404, 121, 40, 291, 9299, 54, 1, 844, 259, 866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thi, gem, film, four, product, the, anticip, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 1229, 7, 698, 288, 1, 2434, 441, 830, 596,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[realli, like, thi, show, drama, romanc, and, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[45, 16, 3, 54, 447, 797, 2, 171, 15, 929, 64,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [base, actual, stori, john, boorman, show, the...          1   \n",
       "1  [thi, gem, film, four, product, the, anticip, ...          1   \n",
       "2  [realli, like, thi, show, drama, romanc, and, ...          1   \n",
       "\n",
       "                                         comment_ids  \n",
       "0  [404, 121, 40, 291, 9299, 54, 1, 844, 259, 866...  \n",
       "1  [3, 1229, 7, 698, 288, 1, 2434, 441, 830, 596,...  \n",
       "2  [45, 16, 3, 54, 447, 797, 2, 171, 15, 929, 64,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[for, movi, that, get, respect, there, sure, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[8, 6, 4, 33, 694, 20, 207, 12, 126, 831, 1649...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bizarr, horror, movi, fill, with, famou, face...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1049, 175, 6, 702, 9, 781, 300, 10, 2269, 684...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[solid, unremark, film, matthau, einstein, won...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1081, 7097, 7, 2611, 4724, 166, 453, 118, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[strang, feel, sit, alon, theater, occupi, par...</td>\n",
       "      <td>1</td>\n",
       "      <td>[551, 117, 500, 580, 581, 3875, 676, 2, 47, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, probabl, all, alreadi, know, thi, now, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13, 223, 15, 459, 84, 3, 130, 10, 1089, 243, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[saw, the, movi, with, two, grown, children, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[199, 1, 6, 9, 89, 2061, 430, 254, 5, 1005, 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[your, use, the, imdb, youv, given, some, heft...</td>\n",
       "      <td>1</td>\n",
       "      <td>[82, 120, 1, 887, 827, 357, 26, 10388, 1365, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[thi, good, film, with, power, messag, love, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 28, 7, 9, 347, 635, 70, 2, 2626, 70, 1, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[made, after, quartet, trio, continu, the, qua...</td>\n",
       "      <td>1</td>\n",
       "      <td>[77, 83, 8771, 3224, 531, 1, 441, 1, 880, 7, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[for, matur, man, admit, that, shed, tear, ove...</td>\n",
       "      <td>1</td>\n",
       "      <td>[8, 1885, 107, 805, 4, 2566, 1157, 109, 3, 7, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [for, movi, that, get, respect, there, sure, a...          1   \n",
       "1  [bizarr, horror, movi, fill, with, famou, face...          1   \n",
       "2  [solid, unremark, film, matthau, einstein, won...          1   \n",
       "3  [strang, feel, sit, alon, theater, occupi, par...          1   \n",
       "4  [you, probabl, all, alreadi, know, thi, now, b...          1   \n",
       "5  [saw, the, movi, with, two, grown, children, a...          1   \n",
       "6  [your, use, the, imdb, youv, given, some, heft...          1   \n",
       "7  [thi, good, film, with, power, messag, love, a...          1   \n",
       "8  [made, after, quartet, trio, continu, the, qua...          1   \n",
       "9  [for, matur, man, admit, that, shed, tear, ove...          1   \n",
       "\n",
       "                                         comment_ids  \n",
       "0  [8, 6, 4, 33, 694, 20, 207, 12, 126, 831, 1649...  \n",
       "1  [1049, 175, 6, 702, 9, 781, 300, 10, 2269, 684...  \n",
       "2  [1081, 7097, 7, 2611, 4724, 166, 453, 118, 2, ...  \n",
       "3  [551, 117, 500, 580, 581, 3875, 676, 2, 47, 13...  \n",
       "4  [13, 223, 15, 459, 84, 3, 130, 10, 1089, 243, ...  \n",
       "5  [199, 1, 6, 9, 89, 2061, 430, 254, 5, 1005, 62...  \n",
       "6  [82, 120, 1, 887, 827, 357, 26, 10388, 1365, 2...  \n",
       "7  [3, 28, 7, 9, 347, 635, 70, 2, 2626, 70, 1, 15...  \n",
       "8  [77, 83, 8771, 3224, 531, 1, 441, 1, 880, 7, 2...  \n",
       "9  [8, 1885, 107, 805, 4, 2566, 1157, 109, 3, 7, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part where we will create and train our Fastext (or another) embedding, or just use already trained one (and comapre results then). \n",
    "Since this is only the first iteration on our project, we are ending here and letting our Neural network create and train the embedding - even if it does not capture the word relations. \n",
    "We can then compare, how using advanced embeddings helps us to achieve better results. \n",
    "\n",
    "##### Project v2 - training embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump text into text file\n",
    "df['comment_text'] = df.comment.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_text = df.comment_text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_input.in', 'w') as write_file:\n",
    "    for text in comment_text:\n",
    "        write_file.write(f'{text}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext embedding offers two types of representation - skipgram and cbow. We chose to train skipgram, since those models should be better at predicting context - and that's what we would like to try to get into our model as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "emb_model = fasttext.train_unsupervised('text_input.in', model='skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.save_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_model = fasttext.load_model(\"fasttext_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.get_word_vector('and').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.zeros((len(vocab) + 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in vocab.items():\n",
    "    embedding[v] = emb_model.get_word_vector(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09602074, -0.35753947, -0.03201434, -0.33081603, -0.0247028 ,\n",
       "         0.08232363, -0.0059217 ,  0.02468209,  0.08795767, -0.18481494,\n",
       "        -0.03276435,  0.28888649, -0.11125953, -0.1052271 ,  0.13869125,\n",
       "         0.11167844, -0.15603273, -0.08394193, -0.01224886,  0.17099385,\n",
       "        -0.28143308, -0.17877622,  0.12219147,  0.00517188, -0.05449152,\n",
       "        -0.02559651, -0.03564524,  0.09680374,  0.05482788, -0.11976379,\n",
       "        -0.158576  , -0.01492924,  0.00803954,  0.10995729,  0.10254253,\n",
       "        -0.31437603, -0.01514904, -0.12867044, -0.09065547, -0.11344571,\n",
       "         0.01505242, -0.19488128, -0.06491195,  0.25242415,  0.24487579,\n",
       "        -0.15954022, -0.12156445, -0.10933612, -0.2566855 , -0.07177238,\n",
       "         0.21224183,  0.11129531, -0.30246639, -0.0419183 ,  0.08962663,\n",
       "        -0.0897903 ,  0.28208923,  0.20725051, -0.14286117, -0.35964763,\n",
       "        -0.07835703, -0.05237986,  0.01240221,  0.16900781,  0.00564027,\n",
       "        -0.12043378, -0.08101835, -0.24174491,  0.07255347,  0.1876263 ,\n",
       "        -0.13805865, -0.18158217, -0.26091328,  0.20065509, -0.29026616,\n",
       "         0.02346004,  0.14049381,  0.05626382, -0.18007341, -0.00088946,\n",
       "        -0.18981817,  0.03808806, -0.27764392,  0.37948123,  0.0677923 ,\n",
       "         0.12051195, -0.17034236, -0.08081543, -0.26484919,  0.1263172 ,\n",
       "        -0.06509201,  0.01429955,  0.22688362, -0.09045897, -0.08472029,\n",
       "         0.04874401,  0.18931118,  0.0674654 , -0.1774496 ,  0.1587915 ],\n",
       "       [ 0.03894049, -0.30279917, -0.10595818, -0.38318917,  0.04144277,\n",
       "         0.00853426, -0.09074514, -0.19818994,  0.16946746, -0.20385753,\n",
       "        -0.0598338 ,  0.3459428 ,  0.08149108,  0.02217242,  0.2697221 ,\n",
       "         0.14211246, -0.2086031 , -0.0462289 ,  0.06411   ,  0.02462793,\n",
       "        -0.21725693,  0.02747092,  0.16194396,  0.15938862,  0.10202292,\n",
       "         0.06928905,  0.10948742,  0.11410914,  0.11255674,  0.05786901,\n",
       "        -0.3457171 ,  0.05887664, -0.0377995 ,  0.15856814,  0.20974122,\n",
       "        -0.31803891, -0.18306035, -0.12434652, -0.00813469, -0.00183493,\n",
       "         0.08235759, -0.02908811,  0.00893705,  0.35541403,  0.29256028,\n",
       "        -0.07270302, -0.11123382, -0.20424214, -0.1132716 , -0.05007121,\n",
       "         0.09246479,  0.0722261 , -0.2452074 ,  0.13178211,  0.21431042,\n",
       "        -0.00419733,  0.36758825,  0.09604856, -0.13901865, -0.29045674,\n",
       "        -0.13582768, -0.13850822,  0.06520347,  0.21650819, -0.00568214,\n",
       "        -0.03992418, -0.01196902, -0.22558554,  0.15687686,  0.11874875,\n",
       "         0.05984311, -0.24547414, -0.35952872,  0.17402215, -0.1970401 ,\n",
       "         0.09067036,  0.18857154,  0.02951174, -0.13842553,  0.07317143,\n",
       "        -0.1742063 ,  0.07952869, -0.38040915,  0.4387618 , -0.03046141,\n",
       "        -0.01709263, -0.08680695, -0.08280563, -0.1515882 ,  0.12368204,\n",
       "        -0.31026784, -0.02021159,  0.14217468,  0.00224297, -0.21599457,\n",
       "        -0.01000878,  0.16809522, -0.10597942, -0.30000222,  0.28669506]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store this embedding and use it within model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word_embeddings.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also still need to make sure that our documents have are the same length. Let's do a quick analysis of lengths first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute comment lengths\n",
    "df['words_n'] = df.comment_ids.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.000000\n",
       "mean       112.184480\n",
       "std         84.741327\n",
       "min          4.000000\n",
       "25%         60.000000\n",
       "50%         83.000000\n",
       "75%        137.000000\n",
       "max       1320.000000\n",
       "Name: words_n, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.words_n.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.words_n.quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.words_n.quantile(0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.words_n.quantile(0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a smaller part of our data set (slightly more than 10\\%) contains more than 200 words. \n",
    "Since we think that 100 words could be enough to detect sentiment, we will set that as maximum length of our comment and cut the words in comments after. However, this is a parameter to play with later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 100 as maximum comment length\n",
    "df['x'] = df.comment_ids.apply(lambda x: x[:100])\n",
    "df_test['x'] = df_test.comment_ids.apply(lambda x: x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad shorter comments with 0\n",
    "df['x'] = df.x.apply(lambda x: np.pad(x, (0, 100 - len(x)), mode='constant'))\n",
    "df_test['x'] = df_test.x.apply(lambda x: np.pad(x, (0, 100 - len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are ready to go for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# length of vocabulary for further processing\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist - to pickle now\n",
    "df.to_pickle('../data/train/comments_embed.pkl')\n",
    "df_test.to_pickle('../data/test/comments_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
