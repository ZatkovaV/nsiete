{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network model\n",
    "\n",
    "Since we are working with text, we choose to train reccurent neural network, LSTM. \n",
    "Our architecture can be described as many-to-one - for many words on input we need to produce one label - 1 for positive and 0 for negative sentiment. \n",
    "\n",
    "Detailed architecture is explained further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f4664dd49d9a91d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f4664dd49d9a91d7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_pickle('../data/train/comments_embed.pkl')\n",
    "test = pd.read_pickle('../data/test/comments_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>words_n</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[movi, get, respect, sure, lot, memor, quot, l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...</td>\n",
       "      <td>29</td>\n",
       "      <td>[1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bizarr, horror, movi, fill, famou, face, stol...</td>\n",
       "      <td>1</td>\n",
       "      <td>[966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...</td>\n",
       "      <td>93</td>\n",
       "      <td>[966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[solid, unremark, film, matthau, einstein, won...</td>\n",
       "      <td>1</td>\n",
       "      <td>[998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...</td>\n",
       "      <td>24</td>\n",
       "      <td>[998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[strang, feel, sit, alon, theater, occupi, par...</td>\n",
       "      <td>1</td>\n",
       "      <td>[473, 60, 424, 502, 503, 3788, 597, 13585, 137...</td>\n",
       "      <td>214</td>\n",
       "      <td>[473, 60, 424, 502, 503, 3788, 597, 13585, 137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[probabl, alreadi, know, addit, episod, never,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...</td>\n",
       "      <td>66</td>\n",
       "      <td>[156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [movi, get, respect, sure, lot, memor, quot, l...          1   \n",
       "1  [bizarr, horror, movi, fill, famou, face, stol...          1   \n",
       "2  [solid, unremark, film, matthau, einstein, won...          1   \n",
       "3  [strang, feel, sit, alon, theater, occupi, par...          1   \n",
       "4  [probabl, alreadi, know, addit, episod, never,...          1   \n",
       "\n",
       "                                         comment_ids  words_n  \\\n",
       "0  [1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...       29   \n",
       "1  [966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...       93   \n",
       "2  [998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...       24   \n",
       "3  [473, 60, 424, 502, 503, 3788, 597, 13585, 137...      214   \n",
       "4  [156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...       66   \n",
       "\n",
       "                                                   x  \n",
       "0  [1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...  \n",
       "1  [966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...  \n",
       "2  [998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...  \n",
       "3  [473, 60, 424, 502, 503, 3788, 597, 13585, 137...  \n",
       "4  [156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare for training\n",
    "train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     8,   615,   140,    67,   751,  1564,   716,  1145,\n",
       "         354,     1,   779, 10299,    63,    79,  5503, 10634,    16,\n",
       "       12978, 12979,     9,   287,   783,    11,  1362, 12980,  6525,\n",
       "         476,  5294,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[base, actual, stori, john, boorman, show, str...</td>\n",
       "      <td>1</td>\n",
       "      <td>[332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...</td>\n",
       "      <td>[332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gem, film, four, product, anticip, qualiti, i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...</td>\n",
       "      <td>[1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[realli, like, show, drama, romanc, comedi, ro...</td>\n",
       "      <td>1</td>\n",
       "      <td>[15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...</td>\n",
       "      <td>[15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[best, experi, disney, themepark, certainli, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...</td>\n",
       "      <td>[51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[korean, movi, ive, seen, three, realli, stuck...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...</td>\n",
       "      <td>[2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [base, actual, stori, john, boorman, show, str...          1   \n",
       "1  [gem, film, four, product, anticip, qualiti, i...          1   \n",
       "2  [realli, like, show, drama, romanc, comedi, ro...          1   \n",
       "3  [best, experi, disney, themepark, certainli, b...          1   \n",
       "4  [korean, movi, ive, seen, three, realli, stuck...          1   \n",
       "\n",
       "                                         comment_ids  \\\n",
       "0  [332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...   \n",
       "1  [1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...   \n",
       "2  [15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...   \n",
       "3  [51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...   \n",
       "4  [2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...   \n",
       "\n",
       "                                                   x  \n",
       "0  [332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...  \n",
       "1  [1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...  \n",
       "2  [15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...  \n",
       "3  [51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...  \n",
       "4  [2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous script, we know that our vocab contains 15000 words and max length of our comment is 100. \n",
    "We also choose our embedding size to be 100 for now - however, these are the hyper-parameters to played with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have pandas dataframe, structure of our data is np.array of np.arrays (not np.ndarray). \n",
    "This might cause problems when training - we need to explicitely convert it to 2d array - one way is using np.stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no good, we need shappe (25000, 100)\n",
    "train.x.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.stack(train.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.stack(test.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target (to make sure we have np arrays)\n",
    "train_y = np.array(train.sentiment.values)\n",
    "test_y = np.array(test.sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first neural network consists of layers:\n",
    "- Embedding layer (to train basic word embedding fror NN to work with) (later we will compare with pretrained embeddings (or train our own embeddings))\n",
    "- Bidirectional LSTM layer (we needed recurrent NN since we work with sequential data - text - so we chose LSTM. WE also went for Bidirectional since we read that it is capable of better understanding of context when making predictions - but there is also a potential to try and use other different architectures. )\n",
    "Size of LSTM layer is also parametrizable - we can try different sizes and compare results - we will start with 64. \n",
    "- Since we need one number at the end - either 1 or 0 (positive or negative sentiment), we needed to add Dense layer to transform our result to such number. For activation function, we chose sigmoid (we were thinking about softmax, but since softmax is just generalized sigmoig (and usable for multiclass classification), we stayed with sigmoid in our problem)\n",
    "\n",
    "Our first NN might be prone to overfitting. In future, we can add for example Dropout layer to try to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v1(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v1, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size))\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v1 = SentimentClassifier_v1(VOCAB_SIZE + 1, EMBEDDING_SIZE, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before compiling our model, we need to choose optimizer. \n",
    "\n",
    "For the first try, we will go with Adam. Next we can try others like SGD.\n",
    "Our loss function is now binary_crossentropy.\n",
    "\n",
    "Our metrics is accuracy. We have balanced dataset (the same number of positive and negative classes) and in such case we think it is an ok metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add callbacks - tensorboard and compile\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v1\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... aaand it is time for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.3703 - accuracy: 0.8371 - val_loss: 0.3541 - val_accuracy: 0.8443\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 109s 4ms/sample - loss: 0.2169 - accuracy: 0.9174 - val_loss: 0.3734 - val_accuracy: 0.8332\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.1251 - accuracy: 0.9545 - val_loss: 0.4537 - val_accuracy: 0.8194\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 106s 4ms/sample - loss: 0.0588 - accuracy: 0.9799 - val_loss: 0.6286 - val_accuracy: 0.8176\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.0300 - accuracy: 0.9899 - val_loss: 0.7007 - val_accuracy: 0.8074\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.0208 - accuracy: 0.9934 - val_loss: 1.0269 - val_accuracy: 0.8202\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.9695 - val_accuracy: 0.8132\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 127s 5ms/sample - loss: 0.0177 - accuracy: 0.9948 - val_loss: 0.9431 - val_accuracy: 0.8086\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 123s 5ms/sample - loss: 0.0074 - accuracy: 0.9976 - val_loss: 1.1229 - val_accuracy: 0.8190\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 115s 5ms/sample - loss: 0.0082 - accuracy: 0.9972 - val_loss: 1.0178 - val_accuracy: 0.8073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95cdb0a400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_v1.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "accuracy on valid: 0.80 afrer 9th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough our first neural network seems to achieve quite fair results, based on the values of train and valid accuracy over the epochs we can tell that our NN is overfitting. \n",
    "\n",
    "We can try to add the Dropout layer to try to reduce overfitting. \n",
    "Other option, how to prevent overfitting, might be to use activity regularizer - l1/l2... we might also think about adding bias to our data - or to even decrease the complexity of network again. We also haven't preformed the hyperparameter tuning yet - which might help to achieve better results as well. \n",
    "\n",
    "And maybe try direct instead of bidirectional LSTM layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v2(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v2, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size, activity_regularizer=l1(0.001)))\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v2 = SentimentClassifier_v2(VOCAB_SIZE + 1, EMBEDDING_SIZE, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 100s 4ms/sample - loss: 0.3802 - accuracy: 0.8374 - val_loss: 0.3615 - val_accuracy: 0.8470\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2177 - accuracy: 0.9214 - val_loss: 0.3904 - val_accuracy: 0.8326\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.1246 - accuracy: 0.9621 - val_loss: 0.4751 - val_accuracy: 0.8192\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 83s 3ms/sample - loss: 0.0678 - accuracy: 0.9828 - val_loss: 0.5505 - val_accuracy: 0.8059\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.0384 - accuracy: 0.9936 - val_loss: 0.7932 - val_accuracy: 0.8080\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 105s 4ms/sample - loss: 0.0268 - accuracy: 0.9973 - val_loss: 0.8514 - val_accuracy: 0.8059\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 97s 4ms/sample - loss: 0.0204 - accuracy: 0.9983 - val_loss: 0.9562 - val_accuracy: 0.8141\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0208 - accuracy: 0.9980 - val_loss: 0.9542 - val_accuracy: 0.7941\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.0173 - accuracy: 0.9990 - val_loss: 0.9949 - val_accuracy: 0.8117\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.0202 - accuracy: 0.9974 - val_loss: 1.0195 - val_accuracy: 0.7999\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 86s 3ms/sample - loss: 0.0202 - accuracy: 0.9974 - val_loss: 1.0184 - val_accuracy: 0.7988\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 93s 4ms/sample - loss: 0.0160 - accuracy: 0.9984 - val_loss: 1.1081 - val_accuracy: 0.7956\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 94s 4ms/sample - loss: 0.0144 - accuracy: 0.9989 - val_loss: 1.0414 - val_accuracy: 0.8000\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.0099 - accuracy: 0.9999 - val_loss: 0.9523 - val_accuracy: 0.8053\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 91s 4ms/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 0.8062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95ae4154e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v2\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v2.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: we keep around 80% accuracy on valid data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that our baseline model is not perfect and overfits. Over epochs, validation loss gets higher (whilst train loss decreases). This is something that we will try to make better during next project iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2.  - custom embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v3(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v3, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size, activity_regularizer=l1(0.001)))\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v2 = SentimentClassifier_v3(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 96s 4ms/sample - loss: 0.6623 - accuracy: 0.6028 - val_loss: 0.6398 - val_accuracy: 0.6359\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.5965 - accuracy: 0.6794 - val_loss: 0.6070 - val_accuracy: 0.6602\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.5381 - accuracy: 0.7314 - val_loss: 0.5260 - val_accuracy: 0.7376\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.4860 - accuracy: 0.7677 - val_loss: 0.5746 - val_accuracy: 0.6995\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 80s 3ms/sample - loss: 0.4402 - accuracy: 0.7976 - val_loss: 0.4731 - val_accuracy: 0.7764\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.4069 - accuracy: 0.8176 - val_loss: 0.4816 - val_accuracy: 0.7641\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3644 - accuracy: 0.8427 - val_loss: 0.4597 - val_accuracy: 0.7936\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.3298 - accuracy: 0.8605 - val_loss: 0.4663 - val_accuracy: 0.7908\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.2994 - accuracy: 0.8766 - val_loss: 0.4933 - val_accuracy: 0.7787\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 87s 3ms/sample - loss: 0.2587 - accuracy: 0.8952 - val_loss: 0.5553 - val_accuracy: 0.7730\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2221 - accuracy: 0.9152 - val_loss: 0.5557 - val_accuracy: 0.7831\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 74s 3ms/sample - loss: 0.1863 - accuracy: 0.9306 - val_loss: 0.5859 - val_accuracy: 0.7881\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 72s 3ms/sample - loss: 0.1490 - accuracy: 0.9490 - val_loss: 0.7165 - val_accuracy: 0.7643\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.1189 - accuracy: 0.9628 - val_loss: 0.7460 - val_accuracy: 0.7770\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.0965 - accuracy: 0.9710 - val_loss: 0.8533 - val_accuracy: 0.7660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2cc9c4b38>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v3\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v2.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
