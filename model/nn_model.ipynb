{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network model\n",
    "\n",
    "Since we are working with text, we choose to train reccurent neural network, LSTM. \n",
    "Our architecture can be described as many-to-one - for many words on input we need to produce one label - 1 for positive and 0 for negative sentiment. \n",
    "\n",
    "Detailed architecture is explained further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_pickle('../data/train/comments_embed.pkl')\n",
    "test = pd.read_pickle('../data/test/comments_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>words_n</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[movi, get, respect, sure, lot, memor, quot, l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...</td>\n",
       "      <td>29</td>\n",
       "      <td>[1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bizarr, horror, movi, fill, famou, face, stol...</td>\n",
       "      <td>1</td>\n",
       "      <td>[966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...</td>\n",
       "      <td>93</td>\n",
       "      <td>[966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[solid, unremark, film, matthau, einstein, won...</td>\n",
       "      <td>1</td>\n",
       "      <td>[998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...</td>\n",
       "      <td>24</td>\n",
       "      <td>[998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[strang, feel, sit, alon, theater, occupi, par...</td>\n",
       "      <td>1</td>\n",
       "      <td>[473, 60, 424, 502, 503, 3788, 597, 13585, 137...</td>\n",
       "      <td>214</td>\n",
       "      <td>[473, 60, 424, 502, 503, 3788, 597, 13585, 137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[probabl, alreadi, know, addit, episod, never,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...</td>\n",
       "      <td>66</td>\n",
       "      <td>[156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [movi, get, respect, sure, lot, memor, quot, l...          1   \n",
       "1  [bizarr, horror, movi, fill, famou, face, stol...          1   \n",
       "2  [solid, unremark, film, matthau, einstein, won...          1   \n",
       "3  [strang, feel, sit, alon, theater, occupi, par...          1   \n",
       "4  [probabl, alreadi, know, addit, episod, never,...          1   \n",
       "\n",
       "                                         comment_ids  words_n  \\\n",
       "0  [1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...       29   \n",
       "1  [966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...       93   \n",
       "2  [998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...       24   \n",
       "3  [473, 60, 424, 502, 503, 3788, 597, 13585, 137...      214   \n",
       "4  [156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...       66   \n",
       "\n",
       "                                                   x  \n",
       "0  [1, 8, 615, 140, 67, 751, 1564, 716, 1145, 354...  \n",
       "1  [966, 109, 1, 624, 701, 228, 2183, 6760, 1478,...  \n",
       "2  [998, 7012, 2, 2525, 4637, 102, 379, 61, 33, 1...  \n",
       "3  [473, 60, 424, 502, 503, 3788, 597, 13585, 137...  \n",
       "4  [156, 385, 35, 1006, 176, 48, 673, 229, 116, 1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare for training\n",
    "train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     8,   615,   140,    67,   751,  1564,   716,  1145,\n",
       "         354,     1,   779, 10299,    63,    79,  5503, 10634,    16,\n",
       "       12978, 12979,     9,   287,   783,    11,  1362, 12980,  6525,\n",
       "         476,  5294,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[base, actual, stori, john, boorman, show, str...</td>\n",
       "      <td>1</td>\n",
       "      <td>[332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...</td>\n",
       "      <td>[332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gem, film, four, product, anticip, qualiti, i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...</td>\n",
       "      <td>[1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[realli, like, show, drama, romanc, comedi, ro...</td>\n",
       "      <td>1</td>\n",
       "      <td>[15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...</td>\n",
       "      <td>[15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[best, experi, disney, themepark, certainli, b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...</td>\n",
       "      <td>[51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[korean, movi, ive, seen, three, realli, stuck...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...</td>\n",
       "      <td>[2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [base, actual, stori, john, boorman, show, str...          1   \n",
       "1  [gem, film, four, product, anticip, qualiti, i...          1   \n",
       "2  [realli, like, show, drama, romanc, comedi, ro...          1   \n",
       "3  [best, experi, disney, themepark, certainli, b...          1   \n",
       "4  [korean, movi, ive, seen, three, realli, stuck...          1   \n",
       "\n",
       "                                         comment_ids  \\\n",
       "0  [332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...   \n",
       "1  [1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...   \n",
       "2  [15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...   \n",
       "3  [51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...   \n",
       "4  [2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...   \n",
       "\n",
       "                                                   x  \n",
       "0  [332, 63, 13, 221, 9212, 18, 764, 190, 786, 54...  \n",
       "1  [1145, 2, 619, 218, 2348, 367, 750, 518, 150, ...  \n",
       "2  [15, 4, 18, 373, 717, 106, 847, 3, 587, 344, 2...  \n",
       "3  [51, 345, 723, 369, 55, 85, 2, 147, 2136, 55, ...  \n",
       "4  [2752, 1, 116, 43, 217, 15, 1382, 27, 207, 109...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous script, we know that our vocab contains 15000 words and max length of our comment is 100. \n",
    "We also choose our embedding size to be 100 for now - however, these are the hyper-parameters to played with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have pandas dataframe, structure of our data is np.array of np.arrays (not np.ndarray). \n",
    "This might cause problems when training - we need to explicitely convert it to 2d array - one way is using np.stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no good, we need shappe (25000, 100)\n",
    "train.x.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.stack(train.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.stack(test.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target (to make sure we have np arrays)\n",
    "train_y = np.array(train.sentiment.values)\n",
    "test_y = np.array(test.sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first neural network consists of layers:\n",
    "- Embedding layer (to train basic word embedding fror NN to work with) (later we will compare with pretrained embeddings (or train our own embeddings))\n",
    "- Bidirectional LSTM layer (we needed recurrent NN since we work with sequential data - text - so we chose LSTM. WE also went for Bidirectional since we read that it is capable of better understanding of context when making predictions - but there is also a potential to try and use other different architectures. )\n",
    "Size of LSTM layer is also parametrizable - we can try different sizes and compare results - we will start with 64. \n",
    "- Since we need one number at the end - either 1 or 0 (positive or negative sentiment), we needed to add Dense layer to transform our result to such number. For activation function, we chose sigmoid (we were thinking about softmax, but since softmax is just generalized sigmoig (and usable for multiclass classification), we stayed with sigmoid in our problem)\n",
    "\n",
    "Our first NN might be prone to overfitting. In future, we can add for example Dropout layer to try to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v1(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v1, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size))\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v1 = SentimentClassifier_v1(VOCAB_SIZE + 1, EMBEDDING_SIZE, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before compiling our model, we need to choose optimizer. \n",
    "\n",
    "For the first try, we will go with Adam. Next we can try others like SGD.\n",
    "Our loss function is now binary_crossentropy.\n",
    "\n",
    "Our metrics is accuracy. We have balanced dataset (the same number of positive and negative classes) and in such case we think it is an ok metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add callbacks - tensorboard and compile\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v1\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0)\n",
    "]\n",
    "\n",
    "nn_v1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... aaand it is time for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.3714 - accuracy: 0.8310 - val_loss: 0.3562 - val_accuracy: 0.8472\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 94s 4ms/sample - loss: 0.2078 - accuracy: 0.9199 - val_loss: 0.4118 - val_accuracy: 0.8362\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 93s 4ms/sample - loss: 0.1152 - accuracy: 0.9582 - val_loss: 0.5014 - val_accuracy: 0.8234\n",
      "Epoch 4/10\n",
      " 9536/25000 [==========>...................] - ETA: 57s - loss: 0.0514 - accuracy: 0.9821"
     ]
    }
   ],
   "source": [
    "nn_v1.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
