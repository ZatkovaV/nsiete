{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network model\n",
    "\n",
    "Since we are working with text, we choose to train reccurent neural network, LSTM. \n",
    "Our architecture can be described as many-to-one - for many words on input we need to produce one label - 1 for positive and 0 for negative sentiment. \n",
    "\n",
    "Detailed architecture is explained further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14156), started 1:16:28 ago. (Use '!kill 14156' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ba3edb96d1dde750\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ba3edb96d1dde750\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_pickle('../data/train/comments_embed.pkl')\n",
    "test = pd.read_pickle('../data/test/comments_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>words_n</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, the, sam...</td>\n",
       "      <td>1</td>\n",
       "      <td>[336, 809, 171, 1915, 1, 148, 27, 26, 48, 1378...</td>\n",
       "      <td>bromwel high cartoon comedi ran the same time ...</td>\n",
       "      <td>98</td>\n",
       "      <td>[336, 809, 171, 1915, 1, 148, 27, 26, 48, 1378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[homeless, houseless, georg, carlin, state, be...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2736, 658, 12488, 591, 60, 835, 8, 96, 10, 97...</td>\n",
       "      <td>homeless houseless georg carlin state been iss...</td>\n",
       "      <td>310</td>\n",
       "      <td>[2736, 658, 12488, 591, 60, 835, 8, 96, 10, 97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[brilliant, overact, lesley, ann, warren, best...</td>\n",
       "      <td>1</td>\n",
       "      <td>[523, 2081, 13669, 1105, 3199, 101, 780, 11953...</td>\n",
       "      <td>brilliant overact lesley ann warren best drama...</td>\n",
       "      <td>112</td>\n",
       "      <td>[523, 2081, 13669, 1105, 3199, 101, 780, 11953...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[thi, easili, the, most, underr, film, inn, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 686, 1, 72, 1846, 7, 8550, 1, 1994, 3971, ...</td>\n",
       "      <td>thi easili the most underr film inn the brook ...</td>\n",
       "      <td>93</td>\n",
       "      <td>[3, 686, 1, 72, 1846, 7, 8550, 1, 1994, 3971, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[thi, not, the, typic, mel, brook, film, much,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 5, 1, 668, 3125, 1994, 7, 56, 341, 2303, 5...</td>\n",
       "      <td>thi not the typic mel brook film much less sla...</td>\n",
       "      <td>95</td>\n",
       "      <td>[3, 5, 1, 668, 3125, 1994, 7, 56, 341, 2303, 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [bromwel, high, cartoon, comedi, ran, the, sam...          1   \n",
       "1  [homeless, houseless, georg, carlin, state, be...          1   \n",
       "2  [brilliant, overact, lesley, ann, warren, best...          1   \n",
       "3  [thi, easili, the, most, underr, film, inn, th...          1   \n",
       "4  [thi, not, the, typic, mel, brook, film, much,...          1   \n",
       "\n",
       "                                         comment_ids  \\\n",
       "0  [336, 809, 171, 1915, 1, 148, 27, 26, 48, 1378...   \n",
       "1  [2736, 658, 12488, 591, 60, 835, 8, 96, 10, 97...   \n",
       "2  [523, 2081, 13669, 1105, 3199, 101, 780, 11953...   \n",
       "3  [3, 686, 1, 72, 1846, 7, 8550, 1, 1994, 3971, ...   \n",
       "4  [3, 5, 1, 668, 3125, 1994, 7, 56, 341, 2303, 5...   \n",
       "\n",
       "                                        comment_text  words_n  \\\n",
       "0  bromwel high cartoon comedi ran the same time ...       98   \n",
       "1  homeless houseless georg carlin state been iss...      310   \n",
       "2  brilliant overact lesley ann warren best drama...      112   \n",
       "3  thi easili the most underr film inn the brook ...       93   \n",
       "4  thi not the typic mel brook film much less sla...       95   \n",
       "\n",
       "                                                   x  \n",
       "0  [336, 809, 171, 1915, 1, 148, 27, 26, 48, 1378...  \n",
       "1  [2736, 658, 12488, 591, 60, 835, 8, 96, 10, 97...  \n",
       "2  [523, 2081, 13669, 1105, 3199, 101, 780, 11953...  \n",
       "3  [3, 686, 1, 72, 1846, 7, 8550, 1, 1994, 3971, ...  \n",
       "4  [3, 5, 1, 668, 3125, 1994, 7, 56, 341, 2303, 5...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare for training\n",
    "train.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  336,   809,   171,  1915,     1,   148,    27,    26,    48,\n",
       "        1378,    23,   391,   102,   119,  1359,    96,     1,  1555,\n",
       "        3746,   238,   154,     4,   336,  1348,    56,  2128,   596,\n",
       "          53,  1359,     1,  8985,   933,  2800,     1,  1533,   813,\n",
       "          17,    29,    37,   176,   124,    47,  1108,  1359, 14390,\n",
       "           1,  3939,     1,   208,   519,    15,   659,     1,   391,\n",
       "         673,     2,    47,   813,    32,   199,     1,   243,    42,\n",
       "         813,  3106,   100,  1015,   167,     1,   391,  1021,  1677,\n",
       "         336,   299,   191,  2523,   105,  5439,    14,    82,  1359,\n",
       "         813,  1782,   336,   212,     4,    90,   697,   419,    68,\n",
       "           4,   336,   222,  4349,    25,  1574,     4,     5,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>comment_ids</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[went, and, saw, thi, movi, last, night, after...</td>\n",
       "      <td>1</td>\n",
       "      <td>[440, 2, 199, 3, 6, 201, 284, 83, 10196, 147, ...</td>\n",
       "      <td>[440, 2, 199, 3, 6, 201, 284, 83, 10196, 147, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[actor, turn, director, bill, paxton, follow, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[93, 157, 127, 769, 4320, 279, 949, 1660, 1, 8...</td>\n",
       "      <td>[93, 157, 127, 769, 4320, 279, 949, 1660, 1, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[recreat, golfer, with, some, knowledg, the, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2592, 9, 26, 1620, 1, 1391, 478, 512, 9, 803,...</td>\n",
       "      <td>[2592, 9, 26, 1620, 1, 1391, 478, 512, 9, 803,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[saw, thi, film, sneak, preview, and, delight,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[199, 3, 7, 2823, 2440, 2, 995, 1, 627, 1413, ...</td>\n",
       "      <td>[199, 3, 7, 2823, 2440, 2, 995, 1, 627, 1413, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bill, paxton, taken, the, true, stori, the, g...</td>\n",
       "      <td>1</td>\n",
       "      <td>[769, 4320, 623, 1, 273, 40, 1, 6595, 318, 2, ...</td>\n",
       "      <td>[769, 4320, 623, 1, 273, 40, 1, 6595, 318, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  sentiment  \\\n",
       "0  [went, and, saw, thi, movi, last, night, after...          1   \n",
       "1  [actor, turn, director, bill, paxton, follow, ...          1   \n",
       "2  [recreat, golfer, with, some, knowledg, the, s...          1   \n",
       "3  [saw, thi, film, sneak, preview, and, delight,...          1   \n",
       "4  [bill, paxton, taken, the, true, stori, the, g...          1   \n",
       "\n",
       "                                         comment_ids  \\\n",
       "0  [440, 2, 199, 3, 6, 201, 284, 83, 10196, 147, ...   \n",
       "1  [93, 157, 127, 769, 4320, 279, 949, 1660, 1, 8...   \n",
       "2  [2592, 9, 26, 1620, 1, 1391, 478, 512, 9, 803,...   \n",
       "3  [199, 3, 7, 2823, 2440, 2, 995, 1, 627, 1413, ...   \n",
       "4  [769, 4320, 623, 1, 273, 40, 1, 6595, 318, 2, ...   \n",
       "\n",
       "                                                   x  \n",
       "0  [440, 2, 199, 3, 6, 201, 284, 83, 10196, 147, ...  \n",
       "1  [93, 157, 127, 769, 4320, 279, 949, 1660, 1, 8...  \n",
       "2  [2592, 9, 26, 1620, 1, 1391, 478, 512, 9, 803,...  \n",
       "3  [199, 3, 7, 2823, 2440, 2, 995, 1, 627, 1413, ...  \n",
       "4  [769, 4320, 623, 1, 273, 40, 1, 6595, 318, 2, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous script, we know that our vocab contains 15000 words and max length of our comment is 100. \n",
    "We also choose our embedding size to be 100 for now - however, these are the hyper-parameters to played with later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have pandas dataframe, structure of our data is np.array of np.arrays (not np.ndarray). \n",
    "This might cause problems when training - we need to explicitely convert it to 2d array - one way is using np.stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no good, we need shappe (25000, 100)\n",
    "train.x.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.stack(train.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.stack(test.x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target (to make sure we have np arrays)\n",
    "train_y = np.array(train.sentiment.values)\n",
    "test_y = np.array(test.sentiment.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first neural network consists of layers:\n",
    "- Embedding layer (to train basic word embedding fror NN to work with) (later we will compare with pretrained embeddings (or train our own embeddings))\n",
    "- Bidirectional LSTM layer (we needed recurrent NN since we work with sequential data - text - so we chose LSTM. WE also went for Bidirectional since we read that it is capable of better understanding of context when making predictions - but there is also a potential to try and use other different architectures. )\n",
    "Size of LSTM layer is also parametrizable - we can try different sizes and compare results - we will start with 64. \n",
    "- Since we need one number at the end - either 1 or 0 (positive or negative sentiment), we needed to add Dense layer to transform our result to such number. For activation function, we chose sigmoid (we were thinking about softmax, but since softmax is just generalized sigmoig (and usable for multiclass classification), we stayed with sigmoid in our problem)\n",
    "\n",
    "Our first NN might be prone to overfitting. In future, we can add for example Dropout layer to try to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v1(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v1, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size))\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v1 = SentimentClassifier_v1(VOCAB_SIZE + 1, EMBEDDING_SIZE, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before compiling our model, we need to choose optimizer. \n",
    "\n",
    "For the first try, we will go with Adam. Next we can try others like SGD.\n",
    "Our loss function is now binary_crossentropy.\n",
    "\n",
    "Our metrics is accuracy. We have balanced dataset (the same number of positive and negative classes) and in such case we think it is an ok metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add callbacks - tensorboard and compile\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v1\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v1.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... aaand it is time for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.3703 - accuracy: 0.8371 - val_loss: 0.3541 - val_accuracy: 0.8443\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 109s 4ms/sample - loss: 0.2169 - accuracy: 0.9174 - val_loss: 0.3734 - val_accuracy: 0.8332\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.1251 - accuracy: 0.9545 - val_loss: 0.4537 - val_accuracy: 0.8194\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 106s 4ms/sample - loss: 0.0588 - accuracy: 0.9799 - val_loss: 0.6286 - val_accuracy: 0.8176\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.0300 - accuracy: 0.9899 - val_loss: 0.7007 - val_accuracy: 0.8074\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.0208 - accuracy: 0.9934 - val_loss: 1.0269 - val_accuracy: 0.8202\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 103s 4ms/sample - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.9695 - val_accuracy: 0.8132\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 127s 5ms/sample - loss: 0.0177 - accuracy: 0.9948 - val_loss: 0.9431 - val_accuracy: 0.8086\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 123s 5ms/sample - loss: 0.0074 - accuracy: 0.9976 - val_loss: 1.1229 - val_accuracy: 0.8190\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 115s 5ms/sample - loss: 0.0082 - accuracy: 0.9972 - val_loss: 1.0178 - val_accuracy: 0.8073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95cdb0a400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_v1.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "accuracy on valid: 0.80 afrer 9th epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough our first neural network seems to achieve quite fair results, based on the values of train and valid accuracy over the epochs we can tell that our NN is overfitting. \n",
    "\n",
    "We can try to add the Dropout layer to try to reduce overfitting. \n",
    "Other option, how to prevent overfitting, might be to use activity regularizer - l1/l2... we might also think about adding bias to our data - or to even decrease the complexity of network again. We also haven't preformed the hyperparameter tuning yet - which might help to achieve better results as well. \n",
    "\n",
    "And maybe try direct instead of bidirectional LSTM layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v2(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v2, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size, activity_regularizer=l1(0.001)))\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v2 = SentimentClassifier_v2(VOCAB_SIZE + 1, EMBEDDING_SIZE, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 100s 4ms/sample - loss: 0.3802 - accuracy: 0.8374 - val_loss: 0.3615 - val_accuracy: 0.8470\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2177 - accuracy: 0.9214 - val_loss: 0.3904 - val_accuracy: 0.8326\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.1246 - accuracy: 0.9621 - val_loss: 0.4751 - val_accuracy: 0.8192\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 83s 3ms/sample - loss: 0.0678 - accuracy: 0.9828 - val_loss: 0.5505 - val_accuracy: 0.8059\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.0384 - accuracy: 0.9936 - val_loss: 0.7932 - val_accuracy: 0.8080\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 105s 4ms/sample - loss: 0.0268 - accuracy: 0.9973 - val_loss: 0.8514 - val_accuracy: 0.8059\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 97s 4ms/sample - loss: 0.0204 - accuracy: 0.9983 - val_loss: 0.9562 - val_accuracy: 0.8141\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0208 - accuracy: 0.9980 - val_loss: 0.9542 - val_accuracy: 0.7941\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.0173 - accuracy: 0.9990 - val_loss: 0.9949 - val_accuracy: 0.8117\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.0202 - accuracy: 0.9974 - val_loss: 1.0195 - val_accuracy: 0.7999\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 86s 3ms/sample - loss: 0.0202 - accuracy: 0.9974 - val_loss: 1.0184 - val_accuracy: 0.7988\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 93s 4ms/sample - loss: 0.0160 - accuracy: 0.9984 - val_loss: 1.1081 - val_accuracy: 0.7956\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 94s 4ms/sample - loss: 0.0144 - accuracy: 0.9989 - val_loss: 1.0414 - val_accuracy: 0.8000\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 92s 4ms/sample - loss: 0.0099 - accuracy: 0.9999 - val_loss: 0.9523 - val_accuracy: 0.8053\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 91s 4ms/sample - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 0.8062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95ae4154e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v2\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v2.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation: we keep around 80% accuracy on valid data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that our baseline model is not perfect and overfits. Over epochs, validation loss gets higher (whilst train loss decreases). This is something that we will try to make better during next project iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2.  - custom embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v3(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v3, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = Bidirectional(LSTM(lstm_size, activity_regularizer=l1(0.001)))\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v2 = SentimentClassifier_v3(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 96s 4ms/sample - loss: 0.6623 - accuracy: 0.6028 - val_loss: 0.6398 - val_accuracy: 0.6359\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.5965 - accuracy: 0.6794 - val_loss: 0.6070 - val_accuracy: 0.6602\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.5381 - accuracy: 0.7314 - val_loss: 0.5260 - val_accuracy: 0.7376\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.4860 - accuracy: 0.7677 - val_loss: 0.5746 - val_accuracy: 0.6995\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 80s 3ms/sample - loss: 0.4402 - accuracy: 0.7976 - val_loss: 0.4731 - val_accuracy: 0.7764\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.4069 - accuracy: 0.8176 - val_loss: 0.4816 - val_accuracy: 0.7641\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3644 - accuracy: 0.8427 - val_loss: 0.4597 - val_accuracy: 0.7936\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.3298 - accuracy: 0.8605 - val_loss: 0.4663 - val_accuracy: 0.7908\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.2994 - accuracy: 0.8766 - val_loss: 0.4933 - val_accuracy: 0.7787\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 87s 3ms/sample - loss: 0.2587 - accuracy: 0.8952 - val_loss: 0.5553 - val_accuracy: 0.7730\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2221 - accuracy: 0.9152 - val_loss: 0.5557 - val_accuracy: 0.7831\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 74s 3ms/sample - loss: 0.1863 - accuracy: 0.9306 - val_loss: 0.5859 - val_accuracy: 0.7881\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 72s 3ms/sample - loss: 0.1490 - accuracy: 0.9490 - val_loss: 0.7165 - val_accuracy: 0.7643\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.1189 - accuracy: 0.9628 - val_loss: 0.7460 - val_accuracy: 0.7770\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.0965 - accuracy: 0.9710 - val_loss: 0.8533 - val_accuracy: 0.7660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2cc9c4b38>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v3\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v2.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying LSTM layer instead of bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v4(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v4, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = LSTM(lstm_size, activity_regularizer=l1(0.001))\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v4 = SentimentClassifier_v4(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.4596 - accuracy: 0.7932 - val_loss: 0.4719 - val_accuracy: 0.7965\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.4031 - accuracy: 0.8254 - val_loss: 0.4761 - val_accuracy: 0.7720\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.3911 - accuracy: 0.8326 - val_loss: 0.4242 - val_accuracy: 0.7987\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.3689 - accuracy: 0.8424 - val_loss: 0.4540 - val_accuracy: 0.8152\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.3556 - accuracy: 0.8493 - val_loss: 0.4592 - val_accuracy: 0.8085\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.3460 - accuracy: 0.8534 - val_loss: 0.3586 - val_accuracy: 0.8410\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.3352 - accuracy: 0.8592 - val_loss: 0.4007 - val_accuracy: 0.8284\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.3258 - accuracy: 0.8620 - val_loss: 0.3500 - val_accuracy: 0.8462\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.3131 - accuracy: 0.8684 - val_loss: 0.3494 - val_accuracy: 0.8462\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.3020 - accuracy: 0.8755 - val_loss: 0.3628 - val_accuracy: 0.8378\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2904 - accuracy: 0.8824 - val_loss: 0.3482 - val_accuracy: 0.8479\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.2801 - accuracy: 0.8848 - val_loss: 0.3518 - val_accuracy: 0.8473\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2662 - accuracy: 0.8922 - val_loss: 0.3635 - val_accuracy: 0.8481\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2530 - accuracy: 0.8988 - val_loss: 0.3928 - val_accuracy: 0.8279\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2419 - accuracy: 0.9033 - val_loss: 0.3846 - val_accuracy: 0.8368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e3b1233cf8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v4\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v4.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v4.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=15,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v5(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v5, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.lstm_layer = LSTM(lstm_size, activity_regularizer=l1(0.001))\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v5 = SentimentClassifier_v5(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 46s 2ms/sample - loss: 0.4662 - accuracy: 0.7872 - val_loss: 0.4287 - val_accuracy: 0.8131\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.4023 - accuracy: 0.8247 - val_loss: 0.4283 - val_accuracy: 0.8092\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.3823 - accuracy: 0.8356 - val_loss: 0.4186 - val_accuracy: 0.8072\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.3668 - accuracy: 0.8436 - val_loss: 0.3631 - val_accuracy: 0.8410\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 42s 2ms/sample - loss: 0.3559 - accuracy: 0.8472 - val_loss: 0.3741 - val_accuracy: 0.8400\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.3411 - accuracy: 0.8532 - val_loss: 0.3656 - val_accuracy: 0.8332\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.3301 - accuracy: 0.8609 - val_loss: 0.3559 - val_accuracy: 0.8462\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.3201 - accuracy: 0.8643 - val_loss: 0.3537 - val_accuracy: 0.8492\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.3083 - accuracy: 0.8716 - val_loss: 0.3606 - val_accuracy: 0.8440\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.2961 - accuracy: 0.8770 - val_loss: 0.3582 - val_accuracy: 0.8416\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.2886 - accuracy: 0.8811 - val_loss: 0.3513 - val_accuracy: 0.8466\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.2722 - accuracy: 0.8895 - val_loss: 0.3530 - val_accuracy: 0.8478\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.2601 - accuracy: 0.8944 - val_loss: 0.3968 - val_accuracy: 0.8484\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 42s 2ms/sample - loss: 0.2468 - accuracy: 0.9022 - val_loss: 0.3685 - val_accuracy: 0.8470\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.2330 - accuracy: 0.9101 - val_loss: 0.3820 - val_accuracy: 0.8471\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.2170 - accuracy: 0.9149 - val_loss: 0.4585 - val_accuracy: 0.8286\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.2032 - accuracy: 0.9221 - val_loss: 0.4229 - val_accuracy: 0.8456\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 42s 2ms/sample - loss: 0.1879 - accuracy: 0.9297 - val_loss: 0.4129 - val_accuracy: 0.8388\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.1717 - accuracy: 0.9371 - val_loss: 0.4512 - val_accuracy: 0.8405\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.1590 - accuracy: 0.9425 - val_loss: 0.4380 - val_accuracy: 0.8373\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.1476 - accuracy: 0.9469 - val_loss: 0.5024 - val_accuracy: 0.8406\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.1308 - accuracy: 0.9550 - val_loss: 0.5094 - val_accuracy: 0.8342\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.1187 - accuracy: 0.9601 - val_loss: 0.5158 - val_accuracy: 0.8332\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.1100 - accuracy: 0.9630 - val_loss: 0.5748 - val_accuracy: 0.8225\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 42s 2ms/sample - loss: 0.0946 - accuracy: 0.9714 - val_loss: 0.6101 - val_accuracy: 0.8327\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.0846 - accuracy: 0.9741 - val_loss: 0.6471 - val_accuracy: 0.8328\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 41s 2ms/sample - loss: 0.0753 - accuracy: 0.9765 - val_loss: 0.6806 - val_accuracy: 0.8303\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.0718 - accuracy: 0.9792 - val_loss: 0.7264 - val_accuracy: 0.8294\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.0635 - accuracy: 0.9824 - val_loss: 0.7393 - val_accuracy: 0.8260\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 40s 2ms/sample - loss: 0.0649 - accuracy: 0.9817 - val_loss: 0.6638 - val_accuracy: 0.8272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x292c818ecc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v5\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v5.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v5.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v6 bidirectional layer with forward and backward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v6(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v6, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.forward_layer = LSTM(lstm_size)\n",
    "        self.backward_layer = LSTM(lstm_size, activation='relu', go_backwards=True)\n",
    "        self.lstm_layer = Bidirectional(self.forward_layer, backward_layer=self.backward_layer)\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v6 = SentimentClassifier_v6(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.6362 - accuracy: 0.7416 - val_loss: 0.6379 - val_accuracy: 0.6871\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 13731.9157 - accuracy: 0.8107 - val_loss: 0.9882 - val_accuracy: 0.5123\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.4803 - accuracy: 0.7681 - val_loss: 0.4146 - val_accuracy: 0.8190\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.3771 - accuracy: 0.8330 - val_loss: 0.3824 - val_accuracy: 0.8286\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.3692 - accuracy: 0.8378 - val_loss: 0.3914 - val_accuracy: 0.8198\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.3597 - accuracy: 0.8395 - val_loss: 0.3610 - val_accuracy: 0.8408\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.3467 - accuracy: 0.8486 - val_loss: 0.3585 - val_accuracy: 0.8430\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.3404 - accuracy: 0.8531 - val_loss: 0.3510 - val_accuracy: 0.8469\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.3297 - accuracy: 0.8575 - val_loss: 0.3490 - val_accuracy: 0.8458\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.3233 - accuracy: 0.8611 - val_loss: 0.3450 - val_accuracy: 0.8496\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.3149 - accuracy: 0.8642 - val_loss: 0.3430 - val_accuracy: 0.8450\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.3043 - accuracy: 0.8688 - val_loss: 0.3382 - val_accuracy: 0.8506\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2954 - accuracy: 0.8735 - val_loss: 0.3412 - val_accuracy: 0.8514\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2864 - accuracy: 0.8787 - val_loss: 0.3391 - val_accuracy: 0.8527\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2726 - accuracy: 0.8841 - val_loss: 0.3494 - val_accuracy: 0.8488\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2615 - accuracy: 0.8899 - val_loss: 0.3520 - val_accuracy: 0.8476\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2468 - accuracy: 0.8946 - val_loss: 0.3587 - val_accuracy: 0.8456\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2335 - accuracy: 0.9019 - val_loss: 0.4298 - val_accuracy: 0.8296\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2195 - accuracy: 0.9089 - val_loss: 0.3921 - val_accuracy: 0.8436\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2029 - accuracy: 0.9172 - val_loss: 0.3947 - val_accuracy: 0.8410\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.1873 - accuracy: 0.9225 - val_loss: 0.4286 - val_accuracy: 0.8458\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1701 - accuracy: 0.9316 - val_loss: 0.4466 - val_accuracy: 0.8348\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1522 - accuracy: 0.9375 - val_loss: 0.4507 - val_accuracy: 0.8394\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1382 - accuracy: 0.9439 - val_loss: 0.4905 - val_accuracy: 0.8343\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1230 - accuracy: 0.9514 - val_loss: 0.5141 - val_accuracy: 0.8363\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.1055 - accuracy: 0.9598 - val_loss: 0.5955 - val_accuracy: 0.8370\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.0989 - accuracy: 0.9629 - val_loss: 0.5958 - val_accuracy: 0.8244\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0856 - accuracy: 0.9667 - val_loss: 0.6388 - val_accuracy: 0.8228\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.0753 - accuracy: 0.9733 - val_loss: 0.6133 - val_accuracy: 0.8220\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.0615 - accuracy: 0.9782 - val_loss: 0.7337 - val_accuracy: 0.8304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x292e519fcc0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v6\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v6.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v6.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v7 lstm_size  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v7(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v7, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.forward_layer = LSTM(lstm_size)\n",
    "        self.backward_layer = LSTM(lstm_size, activation='relu', go_backwards=True)\n",
    "        self.lstm_layer = Bidirectional(self.forward_layer, backward_layer=self.backward_layer)\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v7 = SentimentClassifier_v7(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 173s 7ms/sample - loss: 12130.2005 - accuracy: 0.6914 - val_loss: 0.4596 - val_accuracy: 0.7938\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 167s 7ms/sample - loss: 0.4326 - accuracy: 0.8099 - val_loss: 0.4037 - val_accuracy: 0.8226\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 227s 9ms/sample - loss: 0.4497 - accuracy: 0.8286 - val_loss: 0.3811 - val_accuracy: 0.8307\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 185s 7ms/sample - loss: 0.3667 - accuracy: 0.8389 - val_loss: 0.3813 - val_accuracy: 0.8384\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 187s 7ms/sample - loss: 0.3511 - accuracy: 0.8461 - val_loss: 0.3977 - val_accuracy: 0.8163\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 185s 7ms/sample - loss: 0.3360 - accuracy: 0.8548 - val_loss: 0.3608 - val_accuracy: 0.8411\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 185s 7ms/sample - loss: 0.3251 - accuracy: 0.8583 - val_loss: 0.3425 - val_accuracy: 0.8477\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 177s 7ms/sample - loss: 0.3135 - accuracy: 0.8644 - val_loss: 0.3426 - val_accuracy: 0.8502\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 178s 7ms/sample - loss: 0.3023 - accuracy: 0.8698 - val_loss: 0.3354 - val_accuracy: 0.8530\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 177s 7ms/sample - loss: 0.2907 - accuracy: 0.8756 - val_loss: 0.3483 - val_accuracy: 0.8456\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 176s 7ms/sample - loss: 0.2745 - accuracy: 0.8843 - val_loss: 0.3573 - val_accuracy: 0.8424\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 176s 7ms/sample - loss: 0.2598 - accuracy: 0.8909 - val_loss: 0.3788 - val_accuracy: 0.8345\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 177s 7ms/sample - loss: 0.2394 - accuracy: 0.9001 - val_loss: 0.3773 - val_accuracy: 0.8334\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 3704s 148ms/sample - loss: 0.2179 - accuracy: 0.9105 - val_loss: 0.3908 - val_accuracy: 0.8422\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 172s 7ms/sample - loss: 616.6969 - accuracy: 0.8975 - val_loss: 0.5126 - val_accuracy: 0.7574\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 188s 8ms/sample - loss: 0.2860 - accuracy: 0.8817 - val_loss: 0.4039 - val_accuracy: 0.8250\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 180s 7ms/sample - loss: 0.2059 - accuracy: 0.9153 - val_loss: 0.4108 - val_accuracy: 0.8274\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 185s 7ms/sample - loss: 0.1705 - accuracy: 0.9316 - val_loss: 0.5143 - val_accuracy: 0.8358\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 193s 8ms/sample - loss: 0.1464 - accuracy: 0.9416 - val_loss: 0.4591 - val_accuracy: 0.8338\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 178s 7ms/sample - loss: 0.1284 - accuracy: 0.9517 - val_loss: 0.4962 - val_accuracy: 0.8339\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 179s 7ms/sample - loss: 0.1117 - accuracy: 0.9578 - val_loss: 0.5129 - val_accuracy: 0.8255\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 180s 7ms/sample - loss: 0.0852 - accuracy: 0.9700 - val_loss: 0.6324 - val_accuracy: 0.8182\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 180s 7ms/sample - loss: 0.0723 - accuracy: 0.9744 - val_loss: 0.7069 - val_accuracy: 0.8172\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 180s 7ms/sample - loss: 0.0596 - accuracy: 0.9799 - val_loss: 0.7319 - val_accuracy: 0.8310\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 179s 7ms/sample - loss: 0.0489 - accuracy: 0.9835 - val_loss: 0.7816 - val_accuracy: 0.8327\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 184s 7ms/sample - loss: 0.0419 - accuracy: 0.9866 - val_loss: 0.7600 - val_accuracy: 0.8297\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 192s 8ms/sample - loss: 0.0314 - accuracy: 0.9894 - val_loss: 0.8500 - val_accuracy: 0.8298\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 192s 8ms/sample - loss: 0.0282 - accuracy: 0.9909 - val_loss: 0.9440 - val_accuracy: 0.8245\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 197s 8ms/sample - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.8285 - val_accuracy: 0.8324\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 196s 8ms/sample - loss: 0.0267 - accuracy: 0.9919 - val_loss: 1.0078 - val_accuracy: 0.8207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x292fbf935f8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v7\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v7.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v7.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v8 lstm_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v8(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v8, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.forward_layer = LSTM(lstm_size)\n",
    "        self.backward_layer = LSTM(lstm_size, activation='relu', go_backwards=True)\n",
    "        self.lstm_layer = Bidirectional(self.forward_layer, backward_layer=self.backward_layer)\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v8 = SentimentClassifier_v8(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 107s 4ms/sample - loss: 0.4928 - accuracy: 0.7781 - val_loss: 0.4297 - val_accuracy: 0.8079\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.4044 - accuracy: 0.8200 - val_loss: 0.4044 - val_accuracy: 0.8104\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.3746 - accuracy: 0.8337 - val_loss: 0.3940 - val_accuracy: 0.8324\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.3590 - accuracy: 0.8439 - val_loss: 0.3686 - val_accuracy: 0.8358\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.3554 - accuracy: 0.8443 - val_loss: 0.4086 - val_accuracy: 0.8284\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 68s 3ms/sample - loss: 0.3428 - accuracy: 0.8516 - val_loss: 0.3551 - val_accuracy: 0.8421\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 68s 3ms/sample - loss: 0.3327 - accuracy: 0.8556 - val_loss: 0.3537 - val_accuracy: 0.8439\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 68s 3ms/sample - loss: 0.3237 - accuracy: 0.8599 - val_loss: 0.3659 - val_accuracy: 0.8358\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 69s 3ms/sample - loss: 0.3165 - accuracy: 0.8649 - val_loss: 0.3577 - val_accuracy: 0.8398\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.3089 - accuracy: 0.8664 - val_loss: 0.3579 - val_accuracy: 0.8393\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.3436 - accuracy: 0.8713 - val_loss: 0.3555 - val_accuracy: 0.8458\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.2985 - accuracy: 0.8726 - val_loss: 0.3443 - val_accuracy: 0.8508\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.2895 - accuracy: 0.8786 - val_loss: 0.3489 - val_accuracy: 0.8453\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.2803 - accuracy: 0.8816 - val_loss: 0.3528 - val_accuracy: 0.8449\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 65s 3ms/sample - loss: 0.2751 - accuracy: 0.8841 - val_loss: 0.3935 - val_accuracy: 0.8256\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.2665 - accuracy: 0.8887 - val_loss: 0.3522 - val_accuracy: 0.8502\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.2589 - accuracy: 0.8912 - val_loss: 0.3474 - val_accuracy: 0.8492\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.2516 - accuracy: 0.8957 - val_loss: 0.4352 - val_accuracy: 0.8195\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.2467 - accuracy: 0.8960 - val_loss: 0.3674 - val_accuracy: 0.8404\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.2351 - accuracy: 0.9024 - val_loss: 0.3566 - val_accuracy: 0.8434\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 63s 3ms/sample - loss: 0.2280 - accuracy: 0.9062 - val_loss: 0.3637 - val_accuracy: 0.8467\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 65s 3ms/sample - loss: 0.2204 - accuracy: 0.9085 - val_loss: 0.3957 - val_accuracy: 0.8290\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.2112 - accuracy: 0.9138 - val_loss: 0.3853 - val_accuracy: 0.8370\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.2026 - accuracy: 0.9161 - val_loss: 0.3943 - val_accuracy: 0.8419\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1946 - accuracy: 0.9215 - val_loss: 0.4321 - val_accuracy: 0.8397\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1870 - accuracy: 0.9257 - val_loss: 0.4341 - val_accuracy: 0.8412\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1888 - accuracy: 0.9263 - val_loss: 0.4141 - val_accuracy: 0.8407\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1717 - accuracy: 0.9322 - val_loss: 0.4148 - val_accuracy: 0.8380\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1669 - accuracy: 0.9334 - val_loss: 0.4892 - val_accuracy: 0.8108\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.1557 - accuracy: 0.9393 - val_loss: 0.4546 - val_accuracy: 0.8370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x293155969b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v8\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v8.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v8.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v9 - long training, 100 epochs, learning_rate = 0.02, lstm_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v9(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v9, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.forward_layer = LSTM(lstm_size)\n",
    "        self.backward_layer = LSTM(lstm_size, activation='relu', go_backwards=True)\n",
    "        self.lstm_layer = Bidirectional(self.forward_layer, backward_layer=self.backward_layer)\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v9 = SentimentClassifier_v9(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/100\n",
      "25000/25000 [==============================] - 145s 6ms/sample - loss: 16.1640 - accuracy: 0.7593 - val_loss: 0.4697 - val_accuracy: 0.7915\n",
      "Epoch 2/100\n",
      "25000/25000 [==============================] - 63s 3ms/sample - loss: 0.4000 - accuracy: 0.8236 - val_loss: 0.3981 - val_accuracy: 0.8198\n",
      "Epoch 3/100\n",
      "25000/25000 [==============================] - 63s 3ms/sample - loss: 0.3778 - accuracy: 0.8340 - val_loss: 0.4098 - val_accuracy: 0.8166\n",
      "Epoch 4/100\n",
      "25000/25000 [==============================] - 65s 3ms/sample - loss: 0.3668 - accuracy: 0.8371 - val_loss: 0.3680 - val_accuracy: 0.8379\n",
      "Epoch 5/100\n",
      "25000/25000 [==============================] - 62s 2ms/sample - loss: 0.3561 - accuracy: 0.8436 - val_loss: 0.3683 - val_accuracy: 0.8382\n",
      "Epoch 6/100\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.3468 - accuracy: 0.8507 - val_loss: 0.3590 - val_accuracy: 0.8415\n",
      "Epoch 7/100\n",
      "25000/25000 [==============================] - 63s 3ms/sample - loss: 0.3401 - accuracy: 0.8516 - val_loss: 0.3591 - val_accuracy: 0.8416\n",
      "Epoch 8/100\n",
      "25000/25000 [==============================] - 63s 3ms/sample - loss: 0.3318 - accuracy: 0.8555 - val_loss: 0.3490 - val_accuracy: 0.8462\n",
      "Epoch 9/100\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.3270 - accuracy: 0.8573 - val_loss: 0.3491 - val_accuracy: 0.8465\n",
      "Epoch 10/100\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.3197 - accuracy: 0.8623 - val_loss: 0.3560 - val_accuracy: 0.8428\n",
      "Epoch 11/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.3171 - accuracy: 0.8647 - val_loss: 0.3433 - val_accuracy: 0.8484\n",
      "Epoch 12/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.3112 - accuracy: 0.8643 - val_loss: 0.3479 - val_accuracy: 0.8494\n",
      "Epoch 13/100\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.3058 - accuracy: 0.8682 - val_loss: 0.3437 - val_accuracy: 0.8504\n",
      "Epoch 14/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.3005 - accuracy: 0.8724 - val_loss: 0.3426 - val_accuracy: 0.8520\n",
      "Epoch 15/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2972 - accuracy: 0.8720 - val_loss: 0.3545 - val_accuracy: 0.8486\n",
      "Epoch 16/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2941 - accuracy: 0.8744 - val_loss: 0.3541 - val_accuracy: 0.8441\n",
      "Epoch 17/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.2877 - accuracy: 0.8776 - val_loss: 0.3428 - val_accuracy: 0.8500\n",
      "Epoch 18/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.2813 - accuracy: 0.8810 - val_loss: 0.3444 - val_accuracy: 0.8498\n",
      "Epoch 19/100\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.2802 - accuracy: 0.8812 - val_loss: 0.3436 - val_accuracy: 0.8466\n",
      "Epoch 20/100\n",
      "25000/25000 [==============================] - 60s 2ms/sample - loss: 0.2756 - accuracy: 0.8826 - val_loss: 0.3543 - val_accuracy: 0.8484\n",
      "Epoch 21/100\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.2716 - accuracy: 0.8866 - val_loss: 0.3476 - val_accuracy: 0.8469\n",
      "Epoch 22/100\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.2675 - accuracy: 0.8877 - val_loss: 0.3584 - val_accuracy: 0.8427\n",
      "Epoch 23/100\n",
      "25000/25000 [==============================] - 67s 3ms/sample - loss: 0.2607 - accuracy: 0.8907 - val_loss: 0.3520 - val_accuracy: 0.8452\n",
      "Epoch 24/100\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.2572 - accuracy: 0.8940 - val_loss: 0.3630 - val_accuracy: 0.8438\n",
      "Epoch 25/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.2549 - accuracy: 0.8933 - val_loss: 0.3574 - val_accuracy: 0.8411\n",
      "Epoch 26/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2492 - accuracy: 0.8950 - val_loss: 0.3625 - val_accuracy: 0.8413\n",
      "Epoch 27/100\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2430 - accuracy: 0.8993 - val_loss: 0.3672 - val_accuracy: 0.8472\n",
      "Epoch 28/100\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.2925 - accuracy: 0.8967 - val_loss: 0.3958 - val_accuracy: 0.8336\n",
      "Epoch 29/100\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.2433 - accuracy: 0.9002 - val_loss: 0.3669 - val_accuracy: 0.8454\n",
      "Epoch 30/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.2382 - accuracy: 0.9011 - val_loss: 0.3679 - val_accuracy: 0.8394\n",
      "Epoch 31/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2367 - accuracy: 0.9016 - val_loss: 0.3767 - val_accuracy: 0.8432\n",
      "Epoch 32/100\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.2328 - accuracy: 0.9037 - val_loss: 0.3680 - val_accuracy: 0.8453\n",
      "Epoch 33/100\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.2287 - accuracy: 0.9071 - val_loss: 0.3698 - val_accuracy: 0.8412\n",
      "Epoch 34/100\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.2262 - accuracy: 0.9075 - val_loss: 0.4096 - val_accuracy: 0.8450\n",
      "Epoch 35/100\n",
      "25000/25000 [==============================] - 55s 2ms/sample - loss: 0.2176 - accuracy: 0.9121 - val_loss: 0.3822 - val_accuracy: 0.8401\n",
      "Epoch 36/100\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.2147 - accuracy: 0.9132 - val_loss: 0.3974 - val_accuracy: 0.8420\n",
      "Epoch 37/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.2108 - accuracy: 0.9141 - val_loss: 0.4107 - val_accuracy: 0.8420\n",
      "Epoch 38/100\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.2072 - accuracy: 0.9155 - val_loss: 0.3946 - val_accuracy: 0.8432\n",
      "Epoch 39/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2026 - accuracy: 0.9185 - val_loss: 0.4343 - val_accuracy: 0.8393\n",
      "Epoch 40/100\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.1996 - accuracy: 0.9190 - val_loss: 0.4245 - val_accuracy: 0.8367\n",
      "Epoch 41/100\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.1961 - accuracy: 0.9212 - val_loss: 0.4348 - val_accuracy: 0.8415\n",
      "Epoch 42/100\n",
      "25000/25000 [==============================] - 63s 3ms/sample - loss: 0.1932 - accuracy: 0.9226 - val_loss: 0.4304 - val_accuracy: 0.8378\n",
      "Epoch 43/100\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1887 - accuracy: 0.9253 - val_loss: 0.4428 - val_accuracy: 0.8393\n",
      "Epoch 44/100\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.1817 - accuracy: 0.9274 - val_loss: 0.4479 - val_accuracy: 0.8360\n",
      "Epoch 45/100\n",
      "25000/25000 [==============================] - 58s 2ms/sample - loss: 0.1792 - accuracy: 0.9276 - val_loss: 0.4305 - val_accuracy: 0.8330\n",
      "Epoch 46/100\n",
      "25000/25000 [==============================] - 57s 2ms/sample - loss: 0.1914 - accuracy: 0.9272 - val_loss: 0.4590 - val_accuracy: 0.8394\n",
      "Epoch 47/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1774 - accuracy: 0.9313 - val_loss: 0.4465 - val_accuracy: 0.8352\n",
      "Epoch 48/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1717 - accuracy: 0.9314 - val_loss: 0.4712 - val_accuracy: 0.8340\n",
      "Epoch 49/100\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.1685 - accuracy: 0.9344 - val_loss: 0.4832 - val_accuracy: 0.8365\n",
      "Epoch 50/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1631 - accuracy: 0.9360 - val_loss: 0.4668 - val_accuracy: 0.8322\n",
      "Epoch 51/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.1591 - accuracy: 0.9376 - val_loss: 0.4792 - val_accuracy: 0.8341\n",
      "Epoch 52/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1616 - accuracy: 0.9368 - val_loss: 0.5081 - val_accuracy: 0.8298\n",
      "Epoch 53/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.1535 - accuracy: 0.9406 - val_loss: 0.5032 - val_accuracy: 0.8345\n",
      "Epoch 54/100\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.1496 - accuracy: 0.9421 - val_loss: 0.5460 - val_accuracy: 0.8285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1441 - accuracy: 0.9449 - val_loss: 0.5342 - val_accuracy: 0.8324\n",
      "Epoch 56/100\n",
      "25000/25000 [==============================] - 43s 2ms/sample - loss: 0.1431 - accuracy: 0.9459 - val_loss: 0.5361 - val_accuracy: 0.8311\n",
      "Epoch 57/100\n",
      "25000/25000 [==============================] - 45s 2ms/sample - loss: 0.1393 - accuracy: 0.9458 - val_loss: 0.5760 - val_accuracy: 0.8262\n",
      "Epoch 58/100\n",
      "25000/25000 [==============================] - 47s 2ms/sample - loss: 0.1424 - accuracy: 0.9454 - val_loss: 0.5970 - val_accuracy: 0.8225\n",
      "Epoch 59/100\n",
      "25000/25000 [==============================] - 61s 2ms/sample - loss: 0.1289 - accuracy: 0.9506 - val_loss: 0.5972 - val_accuracy: 0.8284\n",
      "Epoch 60/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.1260 - accuracy: 0.9527 - val_loss: 0.5851 - val_accuracy: 0.8302\n",
      "Epoch 61/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1185 - accuracy: 0.9566 - val_loss: 0.5764 - val_accuracy: 0.8231\n",
      "Epoch 62/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1183 - accuracy: 0.9562 - val_loss: 0.6485 - val_accuracy: 0.8268\n",
      "Epoch 63/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1150 - accuracy: 0.9589 - val_loss: 0.6396 - val_accuracy: 0.8244\n",
      "Epoch 64/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1106 - accuracy: 0.9592 - val_loss: 0.5908 - val_accuracy: 0.8092\n",
      "Epoch 65/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1046 - accuracy: 0.9620 - val_loss: 0.6618 - val_accuracy: 0.8242\n",
      "Epoch 66/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.0969 - accuracy: 0.9655 - val_loss: 0.6643 - val_accuracy: 0.8222\n",
      "Epoch 67/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0967 - accuracy: 0.9641 - val_loss: 0.6827 - val_accuracy: 0.8274\n",
      "Epoch 68/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1018 - accuracy: 0.9628 - val_loss: 0.6057 - val_accuracy: 0.7920\n",
      "Epoch 69/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0952 - accuracy: 0.9661 - val_loss: 0.6881 - val_accuracy: 0.8171\n",
      "Epoch 70/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0812 - accuracy: 0.9729 - val_loss: 0.7494 - val_accuracy: 0.8253\n",
      "Epoch 71/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0805 - accuracy: 0.9716 - val_loss: 0.7653 - val_accuracy: 0.8228\n",
      "Epoch 72/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0878 - accuracy: 0.9682 - val_loss: 0.7868 - val_accuracy: 0.8144\n",
      "Epoch 73/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.0826 - accuracy: 0.9706 - val_loss: 0.7535 - val_accuracy: 0.8163\n",
      "Epoch 74/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.0729 - accuracy: 0.9750 - val_loss: 0.7954 - val_accuracy: 0.8178\n",
      "Epoch 75/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0733 - accuracy: 0.9749 - val_loss: 0.8172 - val_accuracy: 0.8206\n",
      "Epoch 76/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0631 - accuracy: 0.9804 - val_loss: 0.8431 - val_accuracy: 0.8139\n",
      "Epoch 77/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.0691 - accuracy: 0.9766 - val_loss: 0.8323 - val_accuracy: 0.8240\n",
      "Epoch 78/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0593 - accuracy: 0.9799 - val_loss: 0.8321 - val_accuracy: 0.8174\n",
      "Epoch 79/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0591 - accuracy: 0.9802 - val_loss: 0.9373 - val_accuracy: 0.8136\n",
      "Epoch 80/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.0659 - accuracy: 0.9777 - val_loss: 0.8686 - val_accuracy: 0.8179\n",
      "Epoch 81/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0476 - accuracy: 0.9861 - val_loss: 0.9331 - val_accuracy: 0.8166\n",
      "Epoch 82/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.0558 - accuracy: 0.9821 - val_loss: 0.8935 - val_accuracy: 0.8162\n",
      "Epoch 83/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0499 - accuracy: 0.9837 - val_loss: 0.9220 - val_accuracy: 0.8128\n",
      "Epoch 84/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0460 - accuracy: 0.9857 - val_loss: 0.9548 - val_accuracy: 0.8102\n",
      "Epoch 85/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0456 - accuracy: 0.9859 - val_loss: 1.0499 - val_accuracy: 0.8164\n",
      "Epoch 86/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.0529 - accuracy: 0.9831 - val_loss: 0.9736 - val_accuracy: 0.8146\n",
      "Epoch 87/100\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.0394 - accuracy: 0.9880 - val_loss: 1.0186 - val_accuracy: 0.8167\n",
      "Epoch 88/100\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.0342 - accuracy: 0.9901 - val_loss: 1.0534 - val_accuracy: 0.8140\n",
      "Epoch 89/100\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.0420 - accuracy: 0.9874 - val_loss: 1.0547 - val_accuracy: 0.8130\n",
      "Epoch 90/100\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.0399 - accuracy: 0.9876 - val_loss: 1.1308 - val_accuracy: 0.8106\n",
      "Epoch 91/100\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0455 - accuracy: 0.9851 - val_loss: 1.0709 - val_accuracy: 0.8092\n",
      "Epoch 92/100\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.0436 - accuracy: 0.9872 - val_loss: 1.0822 - val_accuracy: 0.8163\n",
      "Epoch 93/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.0535 - accuracy: 0.9845 - val_loss: 1.0380 - val_accuracy: 0.8184\n",
      "Epoch 94/100\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.0281 - accuracy: 0.9924 - val_loss: 1.1549 - val_accuracy: 0.8169\n",
      "Epoch 95/100\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.0363 - accuracy: 0.9895 - val_loss: 1.0917 - val_accuracy: 0.8211\n",
      "Epoch 96/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.0335 - accuracy: 0.9905 - val_loss: 1.1904 - val_accuracy: 0.8141\n",
      "Epoch 97/100\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.0485 - accuracy: 0.9854 - val_loss: 1.2304 - val_accuracy: 0.8096\n",
      "Epoch 98/100\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.0182 - accuracy: 0.9966 - val_loss: 1.2437 - val_accuracy: 0.8079\n",
      "Epoch 99/100\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.0298 - accuracy: 0.9913 - val_loss: 1.1522 - val_accuracy: 0.8128\n",
      "Epoch 100/100\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.0255 - accuracy: 0.9926 - val_loss: 1.1955 - val_accuracy: 0.8130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29324960a58>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v9\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v9.compile(\n",
    "    learning_rate=0.02,\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v9.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v10 -  batch_size = 16 and learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v10(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v10, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.forward_layer = LSTM(lstm_size)\n",
    "        self.backward_layer = LSTM(lstm_size, activation='relu', go_backwards=True)\n",
    "        self.lstm_layer = Bidirectional(self.forward_layer, backward_layer=self.backward_layer)\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v10 = SentimentClassifier_v10(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 96s 4ms/sample - loss: 0.4668 - accuracy: 0.7894 - val_loss: 0.4344 - val_accuracy: 0.8003\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 81s 3ms/sample - loss: 0.4073 - accuracy: 0.8291 - val_loss: 0.4194 - val_accuracy: 0.8060\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.3675 - accuracy: 0.8398 - val_loss: 0.3714 - val_accuracy: 0.8324\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.3575 - accuracy: 0.8441 - val_loss: 0.3659 - val_accuracy: 0.8361\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.3474 - accuracy: 0.8466 - val_loss: 0.3913 - val_accuracy: 0.8295\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 86s 3ms/sample - loss: 0.3363 - accuracy: 0.8549 - val_loss: 0.3497 - val_accuracy: 0.8436\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.3294 - accuracy: 0.8575 - val_loss: 0.4034 - val_accuracy: 0.8168\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 87s 3ms/sample - loss: 0.3193 - accuracy: 0.8632 - val_loss: 0.3730 - val_accuracy: 0.8349\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 81s 3ms/sample - loss: 0.3110 - accuracy: 0.8673 - val_loss: 0.3454 - val_accuracy: 0.8456\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 87s 3ms/sample - loss: 0.3006 - accuracy: 0.8714 - val_loss: 0.3576 - val_accuracy: 0.8402\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 85s 3ms/sample - loss: 0.2924 - accuracy: 0.8749 - val_loss: 0.3458 - val_accuracy: 0.8496\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.2829 - accuracy: 0.8792 - val_loss: 0.3448 - val_accuracy: 0.8501\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2753 - accuracy: 0.8842 - val_loss: 0.3548 - val_accuracy: 0.8444\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.2695 - accuracy: 0.8853 - val_loss: 0.3587 - val_accuracy: 0.8452\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.2606 - accuracy: 0.8919 - val_loss: 0.3532 - val_accuracy: 0.8497\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.2517 - accuracy: 0.8957 - val_loss: 0.3606 - val_accuracy: 0.8390\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.2433 - accuracy: 0.8986 - val_loss: 0.3945 - val_accuracy: 0.8467\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.2383 - accuracy: 0.9018 - val_loss: 0.4020 - val_accuracy: 0.8394\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.2284 - accuracy: 0.9071 - val_loss: 0.3820 - val_accuracy: 0.8426\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.2232 - accuracy: 0.9115 - val_loss: 0.3729 - val_accuracy: 0.8428\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.2133 - accuracy: 0.9143 - val_loss: 0.3938 - val_accuracy: 0.8373\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.2047 - accuracy: 0.9189 - val_loss: 0.4156 - val_accuracy: 0.8327\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.1969 - accuracy: 0.9220 - val_loss: 0.4679 - val_accuracy: 0.8308\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 81s 3ms/sample - loss: 0.1912 - accuracy: 0.9249 - val_loss: 0.4304 - val_accuracy: 0.8371\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.1846 - accuracy: 0.9276 - val_loss: 0.4623 - val_accuracy: 0.8295\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.1760 - accuracy: 0.9314 - val_loss: 0.4818 - val_accuracy: 0.8270\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 88s 4ms/sample - loss: 0.1716 - accuracy: 0.9335 - val_loss: 0.4691 - val_accuracy: 0.8325\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.1619 - accuracy: 0.9383 - val_loss: 0.4634 - val_accuracy: 0.8330\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 79s 3ms/sample - loss: 0.1557 - accuracy: 0.9416 - val_loss: 0.4912 - val_accuracy: 0.8276\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 84s 3ms/sample - loss: 0.1491 - accuracy: 0.9424 - val_loss: 0.4756 - val_accuracy: 0.8269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x209f1e3f048>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v10\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v10.compile(\n",
    "    learning_rate=0.01,\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v10.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=16,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v11 - added activity regulizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.load('word_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_SIZE = 100\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NN architecture\n",
    "class SentimentClassifier_v11(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, embedding_matrix, comment_size, lstm_size):\n",
    "        super(SentimentClassifier_v11, self).__init__()\n",
    "        \n",
    "        # train embedding \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=comment_size,\n",
    "            mask_zero=True, \n",
    "            trainable=False\n",
    "        )\n",
    "    \n",
    "        self.forward_layer = LSTM(lstm_size, activity_regularizer=l1(0.001))\n",
    "        self.backward_layer = LSTM(lstm_size, activation='relu', go_backwards=True, activity_regularizer=l1(0.001))\n",
    "        self.lstm_layer = Bidirectional(self.forward_layer, backward_layer=self.backward_layer)\n",
    "        self.drouput_layer = Dropout(0.5)\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NN object\n",
    "nn_v11 = SentimentClassifier_v11(VOCAB_SIZE + 1, EMBEDDING_SIZE, embedding, COMMENT_SIZE, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/30\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.6697 - accuracy: 0.7765 - val_loss: 0.4373 - val_accuracy: 0.8070\n",
      "Epoch 2/30\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.4062 - accuracy: 0.8234 - val_loss: 0.4198 - val_accuracy: 0.8122\n",
      "Epoch 3/30\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.3831 - accuracy: 0.8348 - val_loss: 0.4039 - val_accuracy: 0.8212\n",
      "Epoch 4/30\n",
      "25000/25000 [==============================] - 64s 3ms/sample - loss: 0.3707 - accuracy: 0.8410 - val_loss: 0.3865 - val_accuracy: 0.8334\n",
      "Epoch 5/30\n",
      "25000/25000 [==============================] - 68s 3ms/sample - loss: 0.3618 - accuracy: 0.8451 - val_loss: 0.3755 - val_accuracy: 0.8340\n",
      "Epoch 6/30\n",
      "25000/25000 [==============================] - 65s 3ms/sample - loss: 0.3543 - accuracy: 0.8489 - val_loss: 0.3632 - val_accuracy: 0.8412\n",
      "Epoch 7/30\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.3434 - accuracy: 0.8544 - val_loss: 0.3601 - val_accuracy: 0.8433\n",
      "Epoch 8/30\n",
      "25000/25000 [==============================] - 66s 3ms/sample - loss: 0.3342 - accuracy: 0.8586 - val_loss: 0.3604 - val_accuracy: 0.8482\n",
      "Epoch 9/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.3231 - accuracy: 0.8646 - val_loss: 0.3695 - val_accuracy: 0.8362\n",
      "Epoch 10/30\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.3154 - accuracy: 0.8664 - val_loss: 0.3566 - val_accuracy: 0.8437\n",
      "Epoch 11/30\n",
      "25000/25000 [==============================] - 46s 2ms/sample - loss: 0.3056 - accuracy: 0.8739 - val_loss: 0.3481 - val_accuracy: 0.8508\n",
      "Epoch 12/30\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.2958 - accuracy: 0.8788 - val_loss: 0.3549 - val_accuracy: 0.8531\n",
      "Epoch 13/30\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.2872 - accuracy: 0.8811 - val_loss: 0.3793 - val_accuracy: 0.8365\n",
      "Epoch 14/30\n",
      "25000/25000 [==============================] - 47s 2ms/sample - loss: 0.2773 - accuracy: 0.8856 - val_loss: 0.3684 - val_accuracy: 0.8468\n",
      "Epoch 15/30\n",
      "25000/25000 [==============================] - 47s 2ms/sample - loss: 0.2730 - accuracy: 0.8881 - val_loss: 0.3717 - val_accuracy: 0.8478\n",
      "Epoch 16/30\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.2621 - accuracy: 0.8930 - val_loss: 0.3687 - val_accuracy: 0.8394\n",
      "Epoch 17/30\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.2500 - accuracy: 0.8993 - val_loss: 0.3718 - val_accuracy: 0.8364\n",
      "Epoch 18/30\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.2419 - accuracy: 0.9026 - val_loss: 0.3886 - val_accuracy: 0.8392\n",
      "Epoch 19/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.2320 - accuracy: 0.9070 - val_loss: 0.4019 - val_accuracy: 0.8367\n",
      "Epoch 20/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.2208 - accuracy: 0.9121 - val_loss: 0.3959 - val_accuracy: 0.8456\n",
      "Epoch 21/30\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.2128 - accuracy: 0.9180 - val_loss: 0.4064 - val_accuracy: 0.8389\n",
      "Epoch 22/30\n",
      "25000/25000 [==============================] - 59s 2ms/sample - loss: 0.2012 - accuracy: 0.9233 - val_loss: 0.4409 - val_accuracy: 0.8246\n",
      "Epoch 23/30\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.1894 - accuracy: 0.9290 - val_loss: 0.4398 - val_accuracy: 0.8347\n",
      "Epoch 24/30\n",
      "25000/25000 [==============================] - 56s 2ms/sample - loss: 0.1792 - accuracy: 0.9323 - val_loss: 0.4760 - val_accuracy: 0.8209\n",
      "Epoch 25/30\n",
      "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.1674 - accuracy: 0.9383 - val_loss: 0.4847 - val_accuracy: 0.8322\n",
      "Epoch 26/30\n",
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.1577 - accuracy: 0.9429 - val_loss: 0.5119 - val_accuracy: 0.8333\n",
      "Epoch 27/30\n",
      "25000/25000 [==============================] - 49s 2ms/sample - loss: 0.1481 - accuracy: 0.9470 - val_loss: 0.5110 - val_accuracy: 0.8340\n",
      "Epoch 28/30\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.1376 - accuracy: 0.9527 - val_loss: 0.5782 - val_accuracy: 0.8131\n",
      "Epoch 29/30\n",
      "25000/25000 [==============================] - 54s 2ms/sample - loss: 0.1277 - accuracy: 0.9565 - val_loss: 0.5416 - val_accuracy: 0.8280\n",
      "Epoch 30/30\n",
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.1230 - accuracy: 0.9588 - val_loss: 0.6000 - val_accuracy: 0.8222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20990f71b38>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(\"logs\", \"sentiment_classifier_v11\"),\n",
    "        histogram_freq=1,\n",
    "        profile_batch=0\n",
    "    )\n",
    "]\n",
    "\n",
    "nn_v11.compile(\n",
    "    learning_rate=0.03,\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "nn_v11.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
